% -*-coding: koi8-r;-*-
% $Id: Part2_NNP.tex,v 1.9 2005-02-10 20:58:07 evlad Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Синтез нейросетевой модели объекта управления}%
\label{nnpsynthesis}

При построении нейросетевой модели реального объекта управления
возникает задача конкретизации архитектуры нейросети и условий её
настройки.  Очевидно, этот этап синтеза нейрорегулятора существенно
зависит от многих факторов: объекта управления, свойств помехи, длины
и прочих свойств имеющихся экспериментальных выборок.  Исследование
этих зависимостей представляет существенный интерес и должно послужить
более ясному представлению о применимости нейросетей в системах
управления.

Нейросетевая модель играет важную роль в обучении НС--Р: она работает
в качестве вычислителя якобиана объекта управления.  Известно, что для
целенаправленного синтеза нейросетевого регулятора достаточно знать
знак якобиана~\cite[с.94]{sigom00}~\cite{wangbao00}; при этом
сходимость алгоритма обучения существенно замедляется.  Однако в
сложных случаях (нестационарный или существенно нелинейный объект)
получить оценку знака якобиана непросто, а ошибка в знаке может
привести к неадекватному управлению и потере устойчивости в
контуре~\cite{wangbao00}.  Учитывая стратегическую важность НС--О,
следует достаточно подробно рассмотреть ее архитектуру, методику
синтеза и особенности функционирования.

\subsubsection{Структура входов нейросети}\label{nnp_inputs}

Как уже отмечалось, выбранная базовая архитектура нейронной сети не
обладает свойством сохранения состояния, а потому нейросеть,
реализующая зависимость $\hat{y}_{k+1}=\NN^o(u_k)$ непосредственно не
позволит создавать модели, адекватные динамическим объектам.

Для построения динамической нейросетевой модели объекта управления
воспользуемся архитектурой с повторением прошлых состояний.  Этот
подход требует решения вспомогательной задачи --- определения, с какой
задержкой и в каком количестве будут подаваться на входы НС--О
значения $u_k, u_{k-1},\ldots,u_{k-D_u}$ и $y_k, \hat{y}_{k-1},\ldots,
\hat{y}_{k-D_y}$.  Очевидно, величина задержки зависит от динамических
свойств объекта управления.  Далее вопрос выбора задержки будет
подробно исследован (п.~\ref{select_DuDy}).

%Поскольку оба случая представляются достаточно распространенными,
%решено отказаться от применения данного подхода к моделированию
%объекта управления.  В противном случае, разрабатываемый метод
%синтеза квазиоптимального нейрорегулятора имел бы более узкую
%область применения.

%%%% Удалил

%Таким образом, в ряде случаев, когда динамические свойства объекта
%управления приблизительно известны и имеется возможность реализовать
%обучение нейрорегулятора в темпе реального времени в системе, вполне
%достаточно ограничиться простой моделью повторения прошлых состояний.
%В дальнейшем будем использовать именно эту модель объекта управления.

\subsubsection{Критерий и цель обучения}%
\label{nnp_criteria_and_goal}

Критерий обучения нейросетевой модели объекта управления --- это
минимизация среднеквадратичной ошибки предсказания.  Однако цель
обучения --- инверсия объекта управления --- отличается от критерия.
Это несовпадение в общем случае может приводить к тому, что не всегда
уменьшение ошибки предсказания будет вести к лучшей инверсии.  В
последующих параграфах будут рассматриваться различные аспекты
достижения критерия обучения НС--О.  Вопрос о том, насколько
построенная согласно критерию нейросетевая модель объекта управления будет
справляться с инверсией объекта, будет поставлен в
п.~\ref{nnp_on_nnc_influence}, так как он связан с задачей синтеза
нейросетевого регулятора.  Там же будут даны дополнительные
рекомендации по выбору архитектуры НС--О.

\subsubsection{Выбор архитектуры нейросети}

Многослойная нейронная сеть рассматриваемого в настоящей работе вида
полностью описывается количеством слоев и числом нейронов в них с
указанием типа нейронов последнего слоя.  Отметим некоторые
особенности выбора архитектуры нейронной сети.

\begin{itemize}

%\item {\it Общее число нейронов}
%
%Известны различные способы оценки необходимого количества нейронов.
%
%\subbbsection{}
%Известная теорема \marginpar{Какая теорема?} о представительной
%способности сети прямого распространения с двумя слоями нейронов с
%сигмоидальной функцией активации~(\figref{fig:nn-classic}) приводит к
%следующему граничному определению количества нейронов скрытого слоя:
%\begin{equation}\label{eq:neuron-number}
%\displaystyle\frac{mN}{(n+m)(1+\log_2N)} \le k \le
%m(N/n+1)+\displaystyle\frac{m}{n+m}(N/n+2)
%\end{equation} где $n$ --- число входов нейросети, $m$ --- число
%выходов (выходных нейронов), $k$ --- число нейронов в скрытом слое,
%$N$ --- количество образов в обучающей выборке.  Количество нейронов,
%меньшее, чем нижний предел, не позволит нейросети успешно распознать
%все образы даже из обучающей выборки.  Слишком большое число нейронов
%(больше верхнего предела), приведет к потере сетью обобщающей
%способности, то есть, вне обучающей выборки результат распознавания
%станет резко хуже, чем для образов из обучающей выборки.
%
%\begin{figure}[h]
%  \centering
%  \input nn_classic.pic
%  \caption{Классическая архитектура многослойного перцептрона.}
%  \label{fig:nn-classic}
%\end{figure}
%
%Однако при малом числе входов и выходов относительно объема обучающей
%выборки имеет место достаточно широкий диапазон значений $k$.
%Рассмотрим типичные величины параметров, входящих
%в~\eqref{eq:neuron-number}, в задаче синтеза нейросетевой модели
%объекта управления.  Например, $n=4$, $m=1$ и $N=500$ дает диапазон
%$10\le k\le227$.
%
%Столь широкий разброс допустимого количества нейронов должен означать
%слабую зависимость представительной способности сети от количества
%нейронов.  Имитационные эксперименты, проведенные с нейросетевой
%моделью управления, подтверждают этот тезис.  Однако всегда следует
%учитывать специфику обучающего множества и уровень ошибки в минимуме,
%полученном в процессе обучения.  Количество весовых коэффициентов
%нейронов и их взаимное расположение влияет на размерность и форму
%поверхности ошибки, по которой проводится градиентный спуск в процессе
%обучения.  Поэтому может оказаться полезным экспериментально
%варьировать число нейронов в сети до получения удовлетворительного
%результата обучения --- нахождения более глубокого локального
%минимума.

\item {\it Количество слоев}

%\subbbsection{}
Исследование архитектуры сетей прямого распространения показало, что
увеличение количества слоев нейронной сети во многих случаях позволяет
ускорить процесс обучения, а также, увеличить точность работы ИНС.
Использование нейросетей с двумя и более скрытыми слоями ставит задачу
распределения нейронов по ним.

Увеличение числа скрытых слоев становится необходимым также в том
случае, если выходные нейроны имеют линейную функцию активации.
Нейросеть является универсальным аппроксиматором любых, в том числе,
нелинейных функций только если число слоев с нелинейной функцией
активации не меньше двух.

\item {\it Распределение нейронов в слоях}

%\subbbsection{}
Известен следующий эвристический метод решения данной задачи.  Скрытый
слой, получающий данные непостредственно с входов ИНС, оснащают числом
нейронов, превышающим число входов (в 1.5--3 раза).  Следующий за ним
скрытый слой имеет меньшее число нейронов, следующий --- еще меньше, и
так далее до последнего скрытого слоя, граничащего с выходным.
Количество нейронов в слоях следует выбирать так, чтобы последний
скрытый слой имел число нейронов больше или равное числу выходных
нейронов.  Общее число нейронов рекомендуется сделать несколько
большим, чем в классической двухслойной нейросети с аналогичными
параметрами.

Первый слой нейронов за счет своего значительного размера позволяет
получить большое число комбинаций исходных признаков.  Последующие
слои последовательно уменьшают число производных признаков, приближая
их к целевому образу.  При рассмотрении нейросети как системы
распознавания, прямое распространение можно интерпретировать как
процесс абстрагирования от всего второстепенного, причем каждый слой
выполняет роль уровня абстракции.  Последний слой является высшим
уровнем абстракции --- он формирует целевой признак, являющийся
результатом распознавания.

\end{itemize}

\subsubsection{Выбор количества и номенклатуры входов нейросети}

%\cite[109,118]{sigom00} - выбор Du, Dy

\label{select_DuDy}
Выбор модели повторения прошлых состояний~\eqref{eq:past-states} в
качестве базовой ставит перед нами проблему выбора чисел $D_u$ и
$D_y$, задающих размер ``входной памяти'', используемой нейросетевой
моделью для предсказания выхода объекта.  Исследуем с помощью
имитационного эксперимента влияние $D_u$ и $D_y$ на качество обучения
нейросетевой модели линейного объекта управления и зависимость
результата обучения от инерционности объекта.

Условия проведения эксперимента:
\begin{itemize}

\item Объект управления: $G^*(z)=\displaystyle\frac{z}{z-d}$, где $d=e^{-1/T}$
      определяет степень инерционности объекта

\item Управляющее воздействие --- н.б.ш. \GaDi{0}{1}
\item Помеха наблюдения --- н.б.ш. \GaDi{0}{0.1}

\item Длина обучающей выборки 250
\item Длина тестовой выборки 500

\item Архитектура НС--О: $\NN^o_{D_u+D_y,8,3,1}$
\item Продолжительность обучения 400 эпох

\end{itemize}

Было проведено $9\times 4\times 4=144$ сеанса обучения со следующими
параметрами объекта и НС--О:

\begin{itemize}
\item $d$ принимает значения 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,
      что соответствует времени установления переходного процесса от
      0.43 до 9.49 дискретных отсчетов времени.
\item $D_u$ принимает значения 1, 2, 3, 4
\item $D_y$ принимает значения 1, 2, 3, 4
\end{itemize}

Результаты эксперимента, сгруппированные по одинаковым значениям
$D_y$, приведены на \figref{fig:nnp_error_DuDy}.  Их анализ позволяет
сделать следующие заключения:

\begin{enumerate}

\item Соблюдение условия $D_u\le D_y$ приводит к меньшей ошибке (в 2--3 раза).

\item При $D_y=1$ зависимость ошибки от инерционности объекта на рассмотренном
      интервале $d$ не проявляется.

\item При $D_y>1$ ошибка увеличивается с увеличением инерционности объекта.

\end{enumerate}

\begin{figure}[h]
\begin{tabular}{p{0.46\textwidth}p{0.46\textwidth}}
\psfig{figure=T1u_D_Du+1_831.ps,angle=270,width=0.46\textwidth} &
\psfig{figure=T1u_D_Du+2_831.ps,angle=270,width=0.46\textwidth}\\[0pt]
\hfil а) $D_y=1$\hfil & \hfil б) $D_y=2$\hfil \\[16pt]
\psfig{figure=T1u_D_Du+3_831.ps,angle=270,width=0.46\textwidth} &
\psfig{figure=T1u_D_Du+4_831.ps,angle=270,width=0.46\textwidth}\\[0pt]
\hfil в) $D_y=3$\hfil & \hfil г) $D_y=4$\hfil \\[16pt]
\end{tabular}
\caption{Зависимость ошибки модели предсказания НС--О от $D_u$ и $d$}
\label{fig:nnp_error_DuDy}
\end{figure}

Очевидно, что увеличение емкости ``входной памяти'' (определяемой
числами $D_u$ и $D_y$) не сказывается существенно на улучшении
качества предсказания нейросетевой моделью выхода объекта управления.
Дальнейшие исследования траектории ошибки в процессе обучения
показали, что добиться меньшей ошибки в 2--3 раза возможно простым
увеличением продолжительности обучения (см. п.\ref{nnp_stat_stability}
и \figref{fig:nnp_case_infl}).  Эксперименты проводились на белом
шуме, используемом в качестве пробного сигнала.  При ином виде
пробного сигнала зависимости, представленные на
\figref{fig:nnp_error_DuDy}, будут несколько иными при сохранении
отмеченных закономерностей.

Обычно вполне достаточным оказывается выбор нейросетевой модели с
$D_u=1$ и $D_y=1$.  Единственный случай, когда можно рекомендовать
$D_y>1$, --- это построение нейросетевой модели объекта управления с
чистым запаздыванием.  В этом случае рекомендуется выбрать величину
$D_y$ не меньше, чем величина чистого запаздывания объекта управления
в дскретах.


\subsubsection{Требования к обучающей и контрольной выборкам}%
\label{nnp_series_req}

По предлагаемой методике обучение нейросетевого имитатора объекта
управления должно осуществляться вне контура управления на выборке,
полученной экспериментальным путем.  Очевидно, временные ряды должны
нести достаточно информации для того, чтобы обучить нейросетевую
модель функционировать с приемлемым качеством.  Встает вопрос о
необходимых требованиях к обучающей и тестовой выборкам $\{u_k,
y_k\}_N$.

В общем случае, выбор идентифицирующего сигнала $u_k$, позволяющего
``изучить'' объект управления, во многом зависит от свойств самого
объекта, а также помехи, неизбежно присутствующей в любой реальной
системе.  Однако реалии функционирующей САУ ограничивают инженера в
выборе идентифицирующего сигнала требованиями допустимости и/или
реализуемости тех или иных возмущающих воздействий.  Как правило,
сложные системы управления имеют встроенные генераторы допустимых
пробных сигналов, используемых при настройке контуров.  Методика
построения промышленных нейросетевых регуляторов должна опираться на
имеющиеся средства, для чего следует изучить, насколько традиционные
пробные сигналы применимы для нейросетевой идентификации.

Детерминистский подход в традиционной теории управления предлагает в
качестве пробных сигналов некоторые ``удобные'' формы, легко
реализуемые физически: гармонический сигнал и ступенчатое воздействие.

С точки зрения линейной теории автоматического управления
гармоническое воздействие позволяет исследовать поведение объекта
только на одной частоте.  Ступенька позволяет получить отклик объекта,
теоретически содержащий все частоты, то есть, исчерпывающе
характеризующий обследуемый объект.  В обоих случаях длина выборки,
содержащей всю полезную информацию, определяется инерционными
свойствами объекта.  Амплитуда (мощность) входного воздействия обычно
берется значительно больше уровня шумов, имеющихся в системе
управления.

Статистические методы исследования линейных систем базируются на
идентифицирующих свойствах случайных сигналов.  Идеальный пробный
сигнал --- белый шум --- на практике недостижим и не всегда применим
из-за физических ограничений на амплитуду возмущающего воздействия.
Обычно бывает достаточно сигнала с известным спектром.

Идентификация объекта управления в статистических методах основывается
на коррелировании входного и выходного сигналов.  Для повышения
помехозащищенности рекомендуется увеличивать длину выборки.  Повышение
мощности входного сигнала также способствует этой цели.

Очевидно, что перечисленные методы идентификации с чистом виде не
применимы к нейронным сетям в силу различия подходов.  Однако имеет
смысл отталкиваться от известных подходов с целью выработки новых.

С целью исследования применимости традиционных пробных сигналов при
обучении нейросетевой модели объекта были проведены многочисленные
имитационные эксперименты со ступенчатым, гармоническим и
стохастическим управляющим воздействием.
%  Обобщая полученный опыт,
%можно сделать некоторые выводы качественного плана.
В качестве примера рассмотрим серию экспериментов с дискретным
объектом управления с передаточной функцией
$G(z)=\frac{0.5z^2}{z^2-0.7}$.  Была выбрана архитектура НС--О
$\NN^o_{1+1,8,3,1}(u_k,y_k)$ с линейной функцией активации
последнего слоя и масштабированием $(-3,3)\to(-1,1)$ на входах и
$(-1,1)\to(-15,15)$ на выходе.

%\subbbsection{Ступенчатый пробный сигнал}
\subsubsection{Ступенчатый пробный сигнал}
Длина обучающей выборки $L=100$.  Форма $0\to 1\to -1$.  Имеются
некоторые особенности, которые необходимо учитывать для успешного
обучения нейросетевой модели:
\begin{itemize}
\item
Длительность обучения $\approx 10^3$ эпох с большим базовым
коэффициентом скорости обучения ($\eta\approx0.1$).
\item
Для того, чтобы нейросеть ``изучила'' как положительную, так и
отрицательную области отклика объекта, рекомендуется использовать
пробный сигнал вида $0\to 1\to -1$
(\figref{fig:nnp_step_training}а).  В случае простой ступеньки с
единственным фронтом $0\to 1$ НС--О будет плохо функционировать в
области отрицательных управляющих воздействий.
\item
В том случае, если амплитуда управляющего сигнала значительно
превышает амплитуду обучающего ступенчатого (в 5--10 раз), качество
функционирования нейросетевой модели резко падает.  На
\figref{fig:nnp_step_training}б заметна потеря качества уже при уровне
сигнала $\approx 4$, а при уровне $\approx 7$ НС--О функционирует
недопустимо плохо.  Таким образом, надежность по амплитуде на примере
составила 4 (максимальное превышение уровня контрольного сигнала над
обучающим при сохранении качества предсказания).
\end{itemize}

\begin{figure}[h]
\begin{tabular}{cc}
\hbox{\psfig{figure=nnp_step_training.ps,angle=270,width=0.45\textwidth,%
             height=0.25\textheight}} &
\hbox{\psfig{figure=nnp_step_trained.ps,angle=270,width=0.45\textwidth,%
             height=0.25\textheight}} \\
а) & б) \\
\end{tabular}
\caption{Обучение НС--О по ступенчатому управляющему воздействию (а)
         и контрольный пример большой амплитуды (б).}
\label{fig:nnp_step_training}
\end{figure}

%\subbbsection{Гармонический пробный сигнал}
\subsubsection{Гармонический пробный сигнал}
Длина обучающей выборки $L=100$.  Период $T=20$.  Амплитуда 1.
Имеются некоторые особенности, которые необходимо учитывать для
успешного обучения нейросетевой модели:
\begin{itemize}
\item
Длительность обучения $\approx 10^3$ эпох с большим базовым
коэффициентом скорости обучения ($\eta\approx0.1$).
\item
Поскольку пробный сигнал имел амплитуду 1, а в качестве теста
подавался сигнал с б\'ольшим размахом (от $-15$ до 8), был обнаружен
эффект падения качества функционирования нейросетевой модели.
Коэффициент надежности по амплитуде примено равен 2.5.
\item
Обучение носит частотно-зависимый характер, то есть, наилучшие
результаты тестирования достигаются на гармоническом сигнале обучающей
частоты.
\item
На ступенчатом возмущении НС--О, построенная по гармоническому
сигналу, дает ошибку коэффициента передачи.
\end{itemize}

\begin{figure}[h]
\begin{tabular}{cc}
\hbox{\psfig{figure=nnp_sin20_training.ps,angle=270,width=0.45\textwidth,%
             height=0.25\textheight}} &
\hbox{\psfig{figure=nnp_sin20_trained.ps,angle=270,width=0.45\textwidth,%
             height=0.25\textheight}} \\
а) & б) \\
\end{tabular}
\caption{Обучение НС--О по моногармоническому воздействию (а)
         и контрольный пример большой амплитуды (б).}
\label{fig:nnp_sin_training}
\end{figure}

%\subbbsection{Стохастический пробный сигнал}
\subsubsection{Стохастический пробный сигнал}
Длина обучающей выборки $L=200$.  Формирующий фильтр
$U(z)=\frac{z}{z-0.95}$.  Имеют место следующие особенности процедуры
обучения и качества функционирования НС--О:
\begin{itemize}
\item
Длительность обучения $\approx 10^3$ эпох с малым базовым
коэффициентом скорости обучения ($\eta\approx0.01$).
\item
Как и с предыдущими пробными сигналами, обучение носит
{амп\-ли\-туд\-но}-зависимый характер.  Коэффициент надежности по амплитуде
примерно равен 1, то есть, нейронная сеть строго ограничена рамками
обучающего множества (\figref{fig:nnp_bad_range}).
\item
В отличие от НС--О, обученных по детерминированным пробным сигналам,
нейросетевая модель, полученная по стохастическому ряду может быть не
выровнена по нулю, то есть, при нулевых входах выход НС--О отличен от
нуля.  Назовем этот отрицательный эффект {\it усилением
нуля.}\label{amplify-zero}
\item
Со значительной ошибкой может определяться коэффициент передачи на
ступенчатом возмущении.
\end{itemize}

\begin{figure}[h]
\centerline{\hbox{\psfig{figure=nnp_bad_range.ps,%
angle=270,width=0.8\textwidth,height=0.3\textheight}}}
\caption{Контрольная выборка значительно выходит за пределы области
         гарантированного качества НС--О.}
\label{fig:nnp_bad_range}
\end{figure}

В двумя последними недостатками настройки НС--О по стохастическому
обучающему множеству можно бороться, выбирая ряды $\{u_k\}_N$ и
$\{y_k\}_N$ так, чтобы $\bar{u}\approx0$ и $\bar{y}\approx0$.  Однако
данный способ не позволяет устранить эти недостатки полностью.

%\subbbsection{Частотные свойства полученных нейросетевых моделей}
\subsubsection{Частотные свойства полученных нейросетевых моделей}
Учитывая, что моделируемый объект обладает частотными свойствами,
исследуем влияние частоты на качество предсказания выхода нейросетевой
модели.  Будем подавать возмущающий моногармонический сигнал
длительности $L=100$ на вход предсказывающей нейросетевой модели,
полученной по одному из рассмотренных опорных сигналов.  Качество
предсказания будем оценивать по величине среднеквадратичной ошибки
(СО).  Зависимость качества (СО) от частоты, полученная в результате
имитационного эксперимента, приведена на
\figref{fig:nnp_mse_freq}.

\begin{figure}[h]
\centerline{\hbox{\psfig{figure=nnp_mse_freq.ps,%
                         angle=270,width=0.8\textwidth,%
                         height=0.3\textheight}}}
\caption{Зависимость среднеквадратичной ошибки предсказания
         нейросетевых моделей от частоты.}
\label{fig:nnp_mse_freq}
\end{figure}

Нейросетевая модель, обученная по гармоническому пробному сигналу без
помехи, имеет частотную характеристику качества в виде несимметричной
параболы с минимумом в точке обучающей частоты.  Начиная с некоторого
уровня помехи ($\sigma=0.1$) данное свойство теряется и частотная
характеристика приобретает монотонно возрастающую форму
(\figref{fig:nnp_freq_test}а).

НС--О, обученная по отклику объекта на ступенчатое возмущение,
проявляет приблизительно линейное ухудшение качества с ростом частоты.
Данное свойство практически не зависит от уровня помехи в канале
наблюдения в изученном диапазоне $\sigma=0\ldots0.3$
(\figref{fig:nnp_freq_test}б).  Интересно отметить, что небольшая
помеха $\sigma=0.05\ldots0.1$ при прочих равных условиях улучшила
качество предсказания гармонического сигнала на низких частотах.

Стохастическая нейросетевая модель обладает аналогичными частотными
свойствами, что и полученная по ступенчатому пробному сигналу.
Качество предсказания моногармонического сигнала практически не
зависит от наличия и мощности помехи при обучении (в диапазоне
$\sigma=0.05\ldots0.3$).

\begin{figure}[h]
\begin{tabular}{cc}
\hbox{\psfig{figure=step_freq_test.ps,angle=270,width=0.45\textwidth,%
             height=0.25\textheight}} &
\hbox{\psfig{figure=sine_freq_test.ps,angle=270,width=0.45\textwidth,%
             height=0.25\textheight}} \\
а) & б)\\
\end{tabular}
\caption{Зависимость среднеквадратичной ошибки предсказания НС--О,
         обученной по ступенчатому (а) и гармоническому (б) пробному
         сигналу, от частоты $f$ и помехи н.б.ш. \GaDi{0}{\sigma} в
         обучающем сигнале.}
\label{fig:nnp_freq_test}
\end{figure}

%\subbbsection{Обобщение результатов экспериментов}%
\subsubsection{Обобщение результатов экспериментов}%
\label{nnp_map_uy}
В целом можно отметить, что все три традиционных пробных сигнала
применимы для обучения нейросетевой модели объекта управления.
Естественно, каждый из них имеет свои недостатки и достоинства.

Может показаться, что ступенчатое возмущающее воздействие является
идеальным для нейросетевой идентификации.  Рассмотрим типичный вид
обучающего множества при ступенчатом пробном сигнале на плоскости
(\figref{fig:nnp_ss_map_uy}).  Ромбами на графике обозначаются
обучающие пары $\{u_k, y_k\}$.  Используя метафору обучения нейронной
сети как апроксимацию функции легко увидеть главный недостаток
ступенчатого сигнала --- отсутствие опорных точек в диапазонах $u$ от
$-1$ до $0$ и от $0$ до $1$.  Эксперимент показал, что для выбранного
линейного объекта управления интерполяция его поведения нейронной
сетью избранной архитектуры оказалась достаточно хорошей.  Более того,
оказалась достаточно хорошей и экстраполяция (надежность по амплитуде
равна 4), что искусственным нейронным сетям несвойственно.  Трудно
сказать, является ли это случайным совпадением или закономерностью.
Однако представляется сомнительным, что объект управления с выраженной
нелинейностью будет хорошо идентифицирован нейросетью на основе
отклика на ступенчатое возмущение.

Единственный способ быть уверенным в поведении НС--О для некоторой
пары $\{u_k, y_k\}$ --- это обучить нейросеть в этой точке.  На
\figref{fig:nnp_ss_map_uy} треугольниками обозначены обучающие пары
стохастического пробного сигнала.  Видно, что в этом случае область
гарантированного при обучении качества предсказания объекта управления
несравненно больше.  Есть уверенность, что при достаточно плотном
покрытии интересующей области в плоскости $u\times y$ окажется
возможным обучать НС--О для нелинейных объектов.

\begin{figure}[h]
\centerline{\hbox{\psfig{figure=nnp_ss_map_uy.ps,%
angle=270,width=0.8\textwidth,height=0.3\textheight}}}
\caption{Обучающие множества при ступенчатом и стохастическом
         пробных сигналах.}
\label{fig:nnp_ss_map_uy}
\end{figure}

Гармонический сигнал с точки зрения равномерности распределения
амплитуд на плоскости $u\times y$ является промежуточным между
ступенчатым и стохастическим.  Обучающие пары распределены по эллипсу
с почти ``пустым'' центром.

% Область гарантированного качества
Итак, наиболее важным моментом при выборе обучающего множества для
построения нейросетевой модели предсказания объекта управления
является правильный выбор амплитуд возмущения и отклика.  Чтобы
нейросеть могла работать в каком-либо диапазоне амплитуд возмущающего
воздействия и наблюдаемого выхода объекта необходимо взять исходную
выборку, охватывающую и заполняющую целевые диапазоны.  Назовем
область, охватываемую обучаемым множеством {\it областью
гарантированного качества.}\label{nnp_guarantee_quality_area}

% Коэффициент надежности по амплитуде
В зависимости от многих факторов (вид обучающей выборки, архитектура
нейросети) обученная нейросеть может обладать способностью без потери
качества функционировать за пределами области гарантированного
качества.  Способность нейросети к экстраполяции можно
охарактеризовать отношением максимальной амплитуды сигнала за
пределами области гарантированного качества, при котором еще не
происходит существенного увеличения ошибки предсказания к максимальной
амплитуде на границе области.  Назовем этот коэффициент {\it
надежностью по амплитуде.}

%\subbbsection{Требования к длине выборок}
\subsubsection{Требования к длине выборок}

Длина обучающей выборки, очевидно, должна быть не короче времени
затухания переходных процессов на объекте.  Данное требование созвучно
линейной теории идентификации.  Вместе с тем, должны быть наложены
дополнительные условия, специфичные для обучения НС--О.

Если есть возможность, желательно сформировать обучающую выборку так,
чтобы реализация содержала амплитуды необходимой величины (то есть,
обеспечивалась желаемая область гарантированного качества) и среднее
значение по амплитуде было возможно близко к нулю.  Последнее
требование, как уже упоминалось выше, может в некоторой степени
устранить ненулевое предсказание выхода объекта при нулевых входах
модели (эффект усиления нуля).

При значительном уровне шумов в системе можно несколько увеличить
длину выборок с целью повышения помехоустойчивости, однако при
высокочастотном шуме и достаточно инерционном объекте ординарной длины
выборки будет вполне достаточно.  Следует предостеречь от чрезмерного
увеличения длины выборки, так как при большем размере обучающего
множества сходимость алгоритма обратного распространения может
существенно уменьшиться и даже прекратиться в локальном минимуме.
Рассмотрим, в каком случае это может произойти; для этого вернемся к
\figref{fig:nnp_ss_map_uy}.

Каждая точка на плоскости имеет значение следующего наблюдаемого
выхода объекта: $(u_k, y_k)\to y_{k+1}$.  Если окажется, что
вследствие шумов близкие точки на плоскости $(u_i, y_i)$ и $(u_j,
y_j)$ будут отображаться в сильно различающиеся значения $y_{i+1}$ и
$y_{j+1}$, то алгоритм обучения будет вынужден сделать негладкую
функцию нейросети в окрестности этих точек, чтобы результирующая
ошибка была минимальна.  При большом объеме зашумленных обучающих
данных число подобных конфликтных областей будет значительным.
Эмпирически известно, что каждая такая область будет требовать
б\'ольших ресурсов сети, чем для воспроизведения гладкой зависимости.
При большом количестве конфликтных областей на определенном шаге
обучающий алгоритм не сможет уменьшить ошибку, так как будет исчерпана
``емкость памяти'' нейронной сети.

К сожалению, на настоящий момент не существует удовлетворительной
количественной теории емкости памяти искусственных нейронных сетей,
поэтому приходится довольствоваться качественными оценками.  Тем не
менее, знание качественных закономерностей позволит инженеру,
проектирующий нейросетевую систему управления, подобрать
удовлетворительную обучающую выборку экспериментальным путем.

В то время, как обучающая выборка должна нести по возможности
исчерпывающую информацию об объекте управления, требования к
контрольной выборке несколько иные.  В частности, ее длина может быть
любой.  Распределение амплитуд пробного сигнала тоже может быть любым,
что позволяет при желании оценить надежность НС--О по амплитуде,
диагностировать величину усиления нуля и прочие нежелательные эффекты.
Главное назначение теста --- это независимая оценка производительности
НС--О в штатных условиях функционирования объекта.  Рекомендуется
проводить тестирование НС--О после каждого обновления весовых
коэффициентов сети в течение всего процесса обучения.  Для оценки
производительности НС--О по контрольной выборке могут применяться
любые критерии, не только среднеквадратичная ошибка.  При начале роста
ошибки на контрольной выборке процесс обучения следует остановить и
считать достигнутое качество функционирования НС--О финальным.

\subsubsection{Статистическая устойчивость процесса обучения}%
\label{nnp_stat_stability}

Исследуем, насколько зависит процесс обучения от реализации обучающей
выборки в том случае, когда пробный сигнал --- случайный.  Для этого
были проведены вычислительные эксперименты, в ходе которых
варьировались исследуемые параметры обучающей и тестовой выборок, а
процесс обучения оценивался по графику среднеквадратичной ошибки (СО)
тестовой выборки.

Эксперименты проводились на НС трех архитектур: однослойной
$\NN^o_{1+3,1}$ (один нейрон), двухслойной
$\NN^o_{1+3,4,1}$ и трехслойной $\NN^o_{1+3,7,3,1}$.
Как видно, на вход НС--О в каждом случае подавались задержанные в
течение трех тактов сигналы с выхода объекта управления $y_k, y_{k-1},
y_{k-2}$ и текущий сигнал управляющего воздействия $u_k$.

В качестве объекта управления было взято инерционное звено с
дискретной передаточной функцией $G(z)=\frac{0.25z}{z-0.75}$.

В эксперименте участвовали 10 различных реализаций выборки
управляющего воздействия и помехи.  И управляющее воздействие, и
помеха, являлись реализациями нормально распределенного псевдобелого
шума с параметрами \GaDi{0}{1} и \GaDi{0}{0.1} соответственно.

Каждая из представленных нейросетей обучалась в течение 400 эпох.  Для
оценки влияния реализации на обучение нейросетевой модели объекта
управления рассматривались траектории СО на тестовой выборке в
процессе обучения.  Длина обучающей и тестовой выборки была взята
равной 500.  Данный объем выборок значительно превышает необходимый
для решения задачи оценивания параметров процесса АРСС и по
соображениям аналогии может считаться достаточным для обучения НС--О.
Равенство длин обучающей и тестовой выборок, а также значительное
превышение их объема над числом настраиваемых параметров нейросети
(весовых коэффициентов) позволяет быть уверенным в отсутствии эффекта
переобучения.

\begin{figure}[h]
\centerline{\hbox{\psfig{figure=T6_mse_1+3_1.eps,%
angle=270,width=0.8\textwidth,height=0.15\textheight}}}
\centerline{а)}
\centerline{\hbox{\psfig{figure=T6_mse_1+3_41.eps,%
angle=270,width=0.8\textwidth,height=0.15\textheight}}}
\centerline{б)}
\centerline{\hbox{\psfig{figure=T6_mse_1+3_731.eps,%
angle=270,width=0.8\textwidth,height=0.15\textheight}}}
\centerline{в)}
\caption{Графики траекторий среднеквадратичной ошибки 10 сеансов
        обучения НС--О с архитектурой $\NN^o_{1+3,1}$ (а),
        $\NN^o_{1+3,4,1}$ (б) и $\NN^o_{1+3,7,3,1}$ (в).}
\label{fig:nnp_case_infl}
\end{figure}

Результаты эксперимента представлены на \figref{fig:nnp_case_infl}.
Графики наглядно демонстрируют слабую зависимость процесса обучения
НС--О от реализации выборок.  Полученные результаты позволяют сделать
следующие эмпирические заключения:

\begin{enumerate}
\item Форма графика СО (число и положение точек перегиба) не
      зависит от реализации выборки, а зависит от архитектуры НС--О;
\item С увеличением числа слоев НС--О форма графика СО усложняется
      и влияние реализации выборки на процесс обучения увеличивается.
\item В худшем случае (трехслойная сеть) отличие минимальной СО от
      максимальной на траектории обучения не превышает 5 раз; в случае
      монотонного уменьшения СО (при отсутствии эффекта переобучения) эта
      разница может быть устранена более продолжительным обучением;
\item С усложнением архитектуры нейронной сети финальная ошибка при
      равной длительности обучения увеличивается.
\end{enumerate}

Закономерный вывод, следующий из результатов экспериментов, ---
использовать настолько простые архитектуры НС--О, насколько это
возможно.
