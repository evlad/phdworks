% -*-coding: cp1251;-*-
% Содержание работы для автореферата

\textbf{Во Введении} обоснована актуальность диссертационной работы,
сформулирована цель и аргументирована научная новизна исследований,
показана практическая значимость полученных результатов, представлены
выносимые на защиту научные положения.

\textbf{В первой главе} на основе публикаций проведен систематический
анализ областей применения нейронных сетей в системах автоматического
управления (САУ).  Отмечено частое использование нейронных сетей (НС)
для управления промышленными технологическими процессами, а также в
робототехнике.  Ключевыми преимуществами нейронных сетей в этих
случаях являются обучение на примерах, не требующее аналитической
идентификации, и нелинейная природа, адекватная нелинейным объектам.

Основными и наиболее перспективными ролями НС общепризнано считается
их использование в качестве регулятора (в том числе, с регулятором
другого типа) и модели объекта управления.  Прочие варианты (настройка
регулятора другого типа, классификация состояний объекта и пр.), судя
по публикациям, используются существенно реже.

При проектировании нейросетевых систем управления возникают следующие
вопросы:
\begin{enumerate}\label{main-nn-questions}
\item Какие входы и выходы и в каком количестве должны быть у НС?
\item Какова должна быть внутренняя архитектура нейросети?
\item Какими данными должна обучаться нейросеть?
\item Какой метод обучения должен использоваться?
\item Когда целесообразно остановить процедуру обучения?
\end{enumerate}

В целом, эти вопросы характерны и для других областей применения НС,
однако однозначных ответов на них в теории нейронных сетей нет, а
имеющиеся теоретически обоснованные подходы к их решению ориентированы
на задачи распознавания, классификации и ассоциативной памяти, сильно
отличающиеся от динамических САУ.  В частности, существуют разные
варианты наборов входов многослойного персептрона, определяющие его
динамические свойства в качестве регулятора и модели объекта, а
формирование обучающего множества не может быть сведено к
распознаванию заранее заданного фиксированного числа образов.

В дополнение к перечисленным вопросам существует нерешенная задача
систематического сопоставления НС с другими подходами, в частности, с
ПИД регуляторами.  В публикациях данная тема, как правило,
затрагивается только с позиций показа преимуществ нейросетевых
регуляторов, в то время как очевидно, что любой подход должен иметь
свои ограничения.

Другим неисследованным объектом сопоставления является линейный
винеровский оптимальный регулятор.  Критерий его синтеза --- минимум
среднеквадратической ошибки управления (СКО) --- совпадает с критерием
обучения нейросетевого регулятора в контуре управления во многих
публикациях как в стационарных, так и нестационарных условиях.  Однако
такого сопоставления никто не проводил, несмотря на его корректность,
а также очевидное научное и практическое значение.

Незавершенность теории НС делает обоснованным использование
разнообразных эвристик, а также применение имитационного эксперимента
как основного метода исследований.  Это характерно для большинства
опубликованных работ.  В то же время, авторы редко аргументируют
причины принятия тех или иных решений, а некоторые аспекты, например,
формирование обучающей выборки, почти всегда остаются за рамками
публикации.

Учитывая эвристический характер проектирования нейросетевых систем
управления, особое значение приобретают программнные инструментальные
средства, позволяющие быстро и удобно настраивать и моделировать такие
системы, а также оценивать качество управления в сравнении с
традиционными подходами.  Для использования в учебном процессе такие
программы должны быть просты в освоении, иметь скромные требования к
производительности компьютера и, что весьма желательно, базироваться
на открытых и бесплатных технологиях.

\textbf{Во второй главе} рассматривается методика синтеза
нейросетевого аналога ПИД регулятора в системе управления с одномерным
линейным объектом.  Пусть $e_k$ --- ошибка управления в момент времени
$t_k$, $u_k$ --- управляющее воздействие на объект, а
$f(e_k,\mathbf{s})$ --- функция ПИД регулятора ($\mathbf{s}$ --- его
состояние).  Для обучения нейросетевого аналога ПИД регулятора
вне контура управления решалась оптимизационная задача вида:
\begin{equation}\label{eq:nnc_synthesis_task}
  \sum\limits_k\big(f(e_k,\mathbf{s})-\NN^p(\mathbf{i}_k)\big)^2
  \rightarrow\min\limits_{\forall\mathbf{i}_k\in\mathbf{I}}
\end{equation}

\noindent где $\NN^p$ --- функция НС со структурой многослойного
персептрона, $\mathbf{i}_k$ --- значения на входах нейросети,
$\mathbf{I}$ --- область возможных значений входов для регулятора.
Данная задача нейросетевой аппроксимации неизвестной таблично заданной
функции $f$ имеет типовое решение градиентным методом (например, {\em
  back-propagation of error}), однако говорить о методике синтеза
можно, только ответив на вопросы, перечисленные
на~\pgref{main-nn-questions}.

Эксперименты с различными наборами входов НС--Р выявили зависимость
ошибки имитации от вида рабочего изменения уставки.  Для постоянной
уставки $r_k=const$ оптимальным выбором являются $e_k\ldots e_{k-d}$ и
$e_k,\Delta e_k$, где $d$ --- длительность переходного процесса в
исходной САУ в отсчетах.  Для переменной уставки наилучший результат
дают входы $r_k,e_k$.  Для кусочно--постоянной уставки наилучшим
является вариант $r_k,e_k\ldots e_{k-d}$.

При использовании правильно подобранной структуры входов зависимости
качества имитации от количества слоев НС--Р и нейронов в них не было
обнаружено.  В этой связи представляется целесообразным использовать
простые двух- и трехслойные сети, теоретически обеспечивающие
аппроксимацию произвольной функции.  Выбор алгоритма обучения НС также
не оказывает существенного влияния на ошибку имитации и управления
НС--Р, а определяет только скорость самого обучения.

В проведенных имитационных экспериментах было обнаружено, что
наилучшим пробным сигналом для формирования обучающего множества
является стохастический ряд, распределенный в диапазоне рабочих
уставок.  Использование моногармонического сигнала дает худший
результат, а ступенчатый пробный сигнал совсем не позволил настроить
НС--Р подобно ПИД.  Важным аспектом при формировании обучающей выборки
является такое распределение амплитуд сигналов на входе и выходе
регулятора, которое охватывает диапазон этих сигналов в основных
рабочих режимах контура.

Анализ результатов экспериментов показал, что малая ошибка имитации
вне контура не всегда обеспечивает малую ошибку управления в контуре
синтезированным НС--Р.  В этом проявляется нелинейная природа НС--Р.
В частности, как уже отмечалось, настройка НС--Р по ступенчатому
пробному сигналу не обеспечивает устойчивое управление в контуре по
причине малой площади покрытия обучающими точками рабочего диапазона.

Отмечено, что синтезированный НС--Р в отличие от ПИД не обеспечивал
нулевую ошибку управления даже по окончании переходного процесса.  Эта
ошибка в проведенных экспериментах не превышала 2\% от ширины рабочего
диапазона уставок.  В то же время, сам переходный процесс под
управлением НС--Р становился короче вследствие отсутствия памяти
состояния и обратных связей у многослойного персептрона.  Также было
отмечено меньшее перерегулирование.

Разработанная методика не опирается на предположение о линейности
объекта, поэтому она была проверена на существенно нелинейном объекте
--- химическом реакторе непрерывного действия с перемешиванием
(continuous stirred-tank reactor).  Синтезованный нейросетевой аналог
ПИД регулятора обеспечил устойчивость контура управления, лучшее
перерегулирование, более короткий переходный процесс, а также меньшую
СКО управления.

\textbf{В третьей главе} предложен метод синтеза нейросетевого
регулятора, минимизирующего СКО управления в контуре.  Данный метод
является развитием косвенного адаптивного управления, однако, в
отличие от него, в качестве начального приближения берется НС--Р,
полученный в результате имитации некоторого линейного регулятора по
методике, изложенной в главе 2.

Критерий синтеза в присутствии аддитивной помехи в канале наблюдения
$n_k$ для объекта с наблюдаемой функцией $g(u_k,\mathbf{s})$
формулируется как:
\begin{equation}\label{eq:noc_synthesis_task}
  \sum\limits_k\big(r_k-g(u_k,\mathbf{s})-n_k\big)^2\rightarrow\min
\end{equation}

\noindent где $u_k=\NN^p(\mathbf{i}_k)$ --- управляющее воздействие
НС--Р.  Приведение ошибки управления к выходу НС--Р можно осуществить
с помощью нейросетевой модели объекта (НС--О) по схеме, показанной
на~\figref{fig:noclearnloop2}.
\begin{figure}[h]
  \centering
  \input{nnplearn.eps_t}
  \caption{Обучение нейросетевого регулятора с помощью инвертирования
  модели объекта.}\label{fig:noclearnloop2}
\end{figure}

Обратное распространение ошибки управления через НС--О к НС--Р,
обозначаемое как $\mathcal{B}^o(u_k, y_{k+1}, r_{k+1})$, неявно
вычисляет якобиан объекта $J_k$:
\begin{equation}\label{eq:nn-jacobinv}
  \mathcal{B}^o(u_k, y_{k+1}, r_{k+1})=\mathsf{z}^{-1}(u_k)+(r_{k+1}-y_k)/J_k
\end{equation}

Задача сформулирована в предположении фиксированной архитектуры НС--Р.
В этом случае, вопросы со~\pgref{main-nn-questions} решались только
для НС--О.  Поскольку нейросетевая модель объекта с обратными связями
имеет известные сложности в использовании (необходим анализ
устойчивости, существует проблема исчезающего градиента при обучении),
за основу был взят многослойный персептрон, реализующий модель
предсказания выхода объекта за счет информации о прошлых выходах
объекта и управляющем воздействии (\figref{fig:nnp-past-states}).
\begin{figure}[h]
  \centering
  \input{nnp_paststates.eps_t}
  \caption{Пример модели с повторением прошлых состояний
  $\hat{y}_{k+1}=\NN^o(u_k, y_k, y_{k-1})$.}
  \label{fig:nnp-past-states}
\end{figure}

Обучение НС--О вне контура управления осуществлялось по критерию:
\begin{equation}\label{eq:nnp_synthesis_task}
  \sum\limits_k\big(g(u_k,\mathbf{s})+n_k-\NN^o(u_k,\ldots,u_{k-n},y_k,\ldots,y_{k-d})\big)^2
  \rightarrow\min\limits
\end{equation}

Эксперименты с устойчивыми минимальнофазовыми линейными объектами
показали, что существенным для обучения НС--О является не порядок
объекта, а его инерционность, то есть, длительность переходного
процесса.  Обучающее множество должно формироваться из временных рядов
длины, не меньшей времени установления переходного процесса объекта.
Глубина регрессии $n=1$, $d=1$ для объектов второго и более высоких
порядков оказалась достаточной, чтобы обеспечить обучение НС--Р в
контуре.


\textbf{В четвертой главе} ...

\textbf{В пятой главе} ...

\textbf{В шестой главе} ...
