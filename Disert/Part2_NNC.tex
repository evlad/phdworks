% $Id: Part2_NNC.tex,v 1.2 2001-12-15 13:56:59 vlad Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Синтез нейросетевого регулятора}\label{nncsynthesis}

Задача обучения нейросетевого регулятора (НС--Р), как уже отмечалось в
п.~\ref{nnplearning}, состоит из двух этапов: получения копии
действующего регулятора вне контура управления и дообучения
функционирующего нейросетевого регулятора в контуре с использованием
инверсной модели объекта.  Будем считать, что в контуре управления
функционирует какой-либо линейный регулятор, например, ПИД.

ПИД регулятор представляет собой динамическую систему, обладающую
памятью предыдущего состояния (кроме простейшего пропорционального
случая управления).  После первого этапа настройки нейросетевой
регулятор должен имитировать функционирование исходного ПИД регулятора
с точностью, достаточной для поддержания системы управления в границах
устойчивости.

Сравнивая задачу построения НС--Р с задачей получения нейросетевой
модели объекта следует отметить два фундаментальный различия.
Во-первых, НС--О может быть реализована как в виде автономной модели
поведения, так и в виде зависимой от объекта модели предсказания, в то
время как НС--Р должен полностью заменить исходный регулятор.
Во-вторых, назначение НС--О --- копирование поведения объекта
управления, а НС--Р должен быть сначала обучен подобно исходному
линейному регулятору, но потом в процессе дообучения в контуре
управления может изменить свои свойства сообразно задаче минимизации
среднеквадратичной ошибки управления.

Как и в случае с синтезом НС--О возникает задача целенаправленного
выбора параметров искусственной нейронной сети и алгоритма ее
обучения.  Очевидно, что реализовать нейросетевой регулятор по тем же
принципам, что и НС--О, не удасться.  Конечно, можно синтезировать
автономную нейросеть с обратными связями, функционирующую подобно
исходному регулятору, а потом дообучать ее.  Однако повторение свойств
линейного регулятора не является основной целью, а лишь предваряет ее,
поэтому не имеет смысла формировать архитектуру нейросетевого
регулятора только исходя из линейности исходного регулятора.  Кроме
того, сохраняет силу аргументация против использования нейронных сетей
с внешними обратными связями, приведенная в п.~\ref{nnp_inputs}.


\subsubsection{Структура входов нейросети}

Рассмотрим несколько вариантов формирования вектора входных значений
для НС--Р.  Обязательно следует проверить вариант подключения,
аналогичный исходному регулятору.  В этом случае на вход подается
только значение ошибки в текущий момент времени $e_k$.  Как было
показано ранее при построении нейросетевой модели объекта, этот
вариант подключения статической нейронной сети не может обеспечить
адекватную имитацию свойств динамической системы.  Чтобы преодолеть
ограничение статического отображения надо искусственно организовать
память с помощью повторения прошлых входов $e_k\ldots e_{k-d}$.
Поэтому исследуем поведение НС--Р при различных значениях емкости
памяти: $0\le d\le 4$.

Рассмотрим также варианты подключения НС--Р с входами $e_k,\Delta e_k$
и $r_k,e_k$.  Последний представляет собой объединение концепций
управления по возмущению $r(t)$ и отклонению $e(t)$.

Для сравнения перечисленных вариантов формирования входного вектора
НС--Р была проведена серия экспериментов.  Ниже приводится постановка
и результаты одного из них.

% nn/dpid.new/contrp/...
Эксперимент проводился в условиях стохастической уставки при наличии
случайной аддитивной помехи в наблюдаемом выходе объекта управления.
Формирующие фильтры уставки $R^*(z)=\frac{0.625z}{z-0.779}$ и помехи
$N^*(z)=0.4$.  Объект $P^*(z)=\frac{z}{z-0.5}$ управлялся ПИД
регулятором $C^*(z)=0.4 + 0.5\frac{z}{z-1} +
0.05\frac{z^2-2z+1}{z(z-1)}$.  Регулятор был настроен на ступенчатом
возмущающем сигнале по критерию минимизации длительности переходного
процесса при максимальном перерегулировании в пределах 4\% амплитуды
возмущающего сигнала.

Нейронная сеть регулятора имела архитектуру $\mathcal{N}^p_{x,7,3,1}$,
где $x$ --- размерность входного вектора.  Обучение осуществлялось на
выборке длиной 200 в течение 400 эпох, а контрольная выборка имела
длину 500, причем если ошибка на контрольной выборке начинала
возрастать, то обучение досрочно прекращалось.  Критерием обучения
являлась минимизация среднеквадратичной ошибки воспроизведения.

\begin{table}[ht]
\caption{Сравнение точности имитации ПИД регулятора вне контура управления
         при различном способе формирования входного вектора НС--Р и
         различных параметрах уставки и помехи}
\label{tabl:nnc_pretr_input_vec}
\begin{tabular}{|c|l|c|c|c|c|}
\hline
\multicolumn{2}{|r|}{Эксперимент} & {\sf А} & {\sf Б} & {\sf В} & {\sf Г}\\
\hline
\multicolumn{2}{|r|}{Уставка} &
  \multicolumn{2}{|c|}{Стохастическая $R^*(z)$} & $\sin(t)$ & $0$ \\
\hline
\multicolumn{2}{|r|}{Помеха} & $N^*(z)=0.4$ & $N^*(z)=0.7$ & $N^*(z)=0.4$ & $N^*(z)=0.4$\\
\hline\hline
N   & Входной & \multicolumn{4}{|c|}{Среднеквадратичная ошибка} \\
пп. & вектор  & \multicolumn{4}{|c|}{на контрольной выборке}\\
\hline
1 & $e_k$                 & 0.3228 & 0.2906 & 0.1299 & 0.0027\\
2 & $e_k,e_{k-1}$         & 0.2993 & 0.2710 & 0.1291 & 0.0022\\
3 & $e_k\ldots e_{k-2}$   & 0.2709 & 0.2459 & 0.1270 & 0.0016\\
4 & $e_k\ldots e_{k-3}$   & 0.2482 & 0.2284 & 0.1240 & 0.0013\\
5 & $e_k\ldots e_{k-4}$   & 0.2385 & 0.2163 & 0.1188 & 0.0013\\
6 & $e_k,\Delta e_k$      & 0.2980 & 0.2738 & 0.1293 & 0.0023\\
7 & $r_k,e_k$             & 0.0388 & 0.0968 & 0.0039 & 0.0040\\
\hline
\end{tabular}
\end{table}

%% $e_k$     0.1296
%% $r_k,e_k$ 0.0037

В таблице~\ref{tabl:nnc_pretr_input_vec} результаты эксперимента
приведены в столбце {\sf А}.  Анализ значений среднеквадратичной
ошибки показывает, что увеличение емкости памяти прошлых входов
позволяет лишь незначительно уменьшить ошибку имитации (строки таблицы
1--5).  Вариант с первой разностью ошибки (строка 6) показывает
близкие результаты с вариантом $e_k,e_{k-1}$ (строка 2).  Очевидно,
что для НС--Р эти варианты информационно эквивалентны.  Наилучшее
качество имитации исходного регулятора было достигнуто НС--Р с входным
вектором $r_k,e_k$, причем достигнутый уровень ошибки в этом случае
оказался меньше почти на порядок, чем в остальных.

Эксперименты показали, что при увеличении мощности помехи
относительное преимущество НС--Р с входным вектором $r_k,e_k$
уменьшается (столбец {\sf Б} таблицы).  Таким образом, применение в
качестве входного вектора $e_k\ldots e_{k-d}$ может оказаться в
некоторых случаях оправданным.

Были также рассмотрены случаи предварительного обучения НС--Р по
детерминированной уставке различной формы.  По результатам
экспериментов наилучшим вариантом формирования входного вектора с
большим отрывом оказалось совмещенное управление по возмущению и
отклонению: $r_k,e_k$ (строка 6).  Оказалось также, что результирующий
уровень ошибки при периодической уставке практически не зависит от ее
формы.  В частности, числовые значения среднеквадратичной ошибки на
гармоническом сигнале (столбец {\sf В}
таблицы~\ref{tabl:nnc_pretr_input_vec}) и на меандре примерно
одинаковы.

Отдельно был рассмотрен случай задачи стабилизации, когда уставка
постоянна, например, равна 0 (столбец {\sf Г}
таблицы~\ref{tabl:nnc_pretr_input_vec}).  Нейросетевой регулятор с
входным вектором $r_k,e_k$ показал худший результат, чем любой из
вариантов с повторением прошлых значений ошибки.  Видно, что
отсутствие влияния уставки на нейронную сеть в данном случае делает
наличие входа $r_k$ не только бесполезным, но и вредным.

Проводились также эксперименты с объектом управления с чистым
запаздыванием и объектом управления второго порядка, анализ которых
обнаружил те же закономерности.

По результатам проведенных экспериментов можно сформулировать
некоторые рекомендации по формированию входного вектора нейросетевого
регулятора по критерию наилучшей имитации исходного ПИД регулятора:

\begin{itemize}
\item
В случае задачи стабилизации (постоянное значение уставки)
рекомендуется использовать один из вариантов с повторением прошлых
значений: $e_k\ldots e_{k-d}$.  При увеличении емкости памяти прошлых
значений (параметр $d$) качество имитации при наличии помехи будет
возрастать.

\item
В случае изменяющегося значения уставки предпочтительным является
совмещенное управление по возмущению и отклонению: $r_k,e_k$.  При
значительном уровне помехи улучшить качество имитации можно вводя и
увеличивая память прошлых состояний.  Входной вектор в этом случае
будет иметь вид $r_k,e_k\ldots e_{k-d}$
\end{itemize}

%%fig:pid_nnc_step_test.ps


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Архитектура нейронной сети регулятора}

Расмотрим задачу выбора внутренней архитектуры нейронной сети
регулятора учитывая ее влияние как на качество имитации исходного
регулятора, так и на дальнейшее дообучение НС--Р в контуре управления.

По результатам нескольких имитационных экспериментов было выяснено,
что предварительное обучение НС--Р дает удовлетворительный результат
уже в случае использования простейшей архитектуры из одного нейрона с
линейной функцией активации и двумя входами $r_k$ и $e_k$.  Очевидно,
что функция такой сети имеет вид $w_0+w_1 r_k+w_2 e_k$.  Графическое
изображение такой функции --- плоскость в трехмерном пространстве с
осями $e(t), r(t), u(t)$.  Анализ показал, что множество обучающих
точек $(e_k, r_k, u_k)$, полученных при функционировании в контуре
исходного ПИД регулятора, имеет форму эллипсоида, сильно сжатого по
оси $u(t)$ и немного наклоненного.

Наилучшей поверхностью, аппроксимирующей облако такой формы будет
наклонная плоскость.  Это объясняет и выбор вектора входов $r_k,e_k$,
и простую архитектуру, необходимую для воспроизведения поведения ПИД
регулятора.

Тем не менее, следует учитывать, что предварительное обучение НС--Р
является только первым этапом синтеза нейросетевого регулятора.
Необходимо установить, какая архитектура НС--Р является достаточной
для последующего обучения в контуре.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Требования к обучающей выборке}








Были проведены эксперименты,

Однако следует 

Такое множество точек лучше всего аппроксимируется плоскостью 

позволил установить типичную форму



Исследуем в имитационном эксперименте влияние сложности архитектуры на
качество синтезируемого НС--Р.


Качество обучения НС--Р с различными входными векторами предлагается
оценить по 

Результаты обучения нейросетевых регуляторов одинаковой архитектуры с
перечисленными вариантами входных векторов приводятся ниже:


  ПИД регулятор
представляет собой динамическую систему $u_k=f(e_k)$ с памятью (кроме
простейшего пропорционального регулятора, не обладающего памятью).
%Как уже отмечалось (п.~\ref{nnp_inputs}), имитация памяти требует
%введения обратных связей.
Анализ зависимости $u_k$ от $e_k$ для ПИД регуляторов в различных
системах управления показывает недостаточность одного лишь значения
ошибки $e_k$ для аппроксимации нейросетью выхода $u_k$ ПИД регулятора.
Например, для системы управления с линейным объектом управления
инерционного типа в условиях стохастической уставки и случайной помехи
в канале наблюдения распределение точек $(u_k,e_k)$ на плоскости
приводится на \figref{fig:pid_eu}.

\begin{figure}[h]
\centerline{\hbox{\psfig{figure=pid_eu.ps,%
angle=270,width=0.8\textwidth,height=0.3\textheight}}}
\caption{Распределение точек $(u_k,e_k)$ для ПИД регулятора.}
\label{fig:pid_eu}
\end{figure}

Исследуем зависимость $u_k$ от $e_k$ для ПИД регулятора, включенного в
контур управления линейным объектом.  Анализ показывает, что влияние
динамической природы регулятора на 

Анализ зависимости $u_k$ от $e_k$ для ПИД регулятора в контуре системы
управления показывает значительное влияние памяти предыдущего
состояния регулятора (\figref{}).



Даже простейший анализ


Дифференциальная часть регулятора в дискретной форме
представляет собой 

Проанализируем возможность нейросетевой имитации ПИД закона
управления:

\begin{equation}\label{pid-continuous}
u(t)=K_P e(t)+K_I\int\limits_0^T e(t)dt+K_D\frac{de(t)}{dt}
\end{equation}

Первое слагаемое $K_P e(t)$ реализует пропорциональную зависимость
управления от ошибки в текущий момент времени.  Данная зависимость
легко реализуется нейронной сетью.

Второе слагаемое $K_I\int\limits_0^T e(t)dt$ усиливает управляющее
воздействие в случае наличия ошибки на протяжении некоторого времени.
Таким образом, память позволяет ускорить 

Третье слагаемое $K_D\frac{de(t)}{dt}$ определяет реакцию регулятора
на изменение ошибки.  В дискретном случае производная обычно
представляется разностью $\Delta e_k=e_k-e_{k-1}$.

Рассмотрим альтернативный способ реализации 

Из традиционной теории управления известны два способа реализации
регулирования: по возмущению и по отклонению.  В линейной ТАУ первый
способ в чистом виде практически не используется, так как в нем не
задействуется информация о фактическом состоянии объекта управления.
Неточность идентификации объекта управления, наличие неконтролируемых
возмущений и помех обычно приводят к неудовлетворительному качеству
управления по возмущению.  Нейросетевые регуляторы, основанные на этом
принципе (последовательная схема нейронного управления в обзоре
\cite{sigom00}), эксплуатируют возможность инверсии объекта
искусственной нейронной сетью после обучения.  Однако недостатки,
присущие принципу регулирования по возмущению остаются и в случае его
нейросетевой реализации.

Гораздо более распространено регулирование по отклонению.  В линейной
теории управления особое внимание уделяется специальному типу
регуляторов, которые реализуют суперпозицию пропорциональной,
дифференциальной или интегральной зависимости управляющего воздействия
от входной ошибки.  ПИД регуляторы широко используется в
промышленности, так как они позволяют решать задачи стабилизации и
слежения в хорошо линеаризуемых детерминированных линейных системах
управления.

\begin{equation}
u_k-u_{k-1}=\Delta u_k=K_P(e_k-e_{k-1})+K_I e_k+K_D(e_k-2e_{k-1}+e_{k-2})
\end{equation}


\begin{equation}\label{eq:reference}
R^*(z)=\displaystyle\frac{0.625z}{z-0.779}
\end{equation}

\begin{equation}\label{eq:noise}
N^*(z)=0.7
\end{equation}

\begin{equation}\label{eq:plant}
P^*(z)=\displaystyle\frac{z}{z-0.5}
\end{equation}

\begin{equation}\label{eq:contr-pid}
C_{PID}^*(z)=0.4 +
       0.5\displaystyle\frac{z}{z-1} +
       0.05\displaystyle\frac{z^2-2z+1}{z(z-1)}
\end{equation}

\begin{equation}\label{eq:contr-wiener}
C_W^*(z)=\displaystyle\frac{0.528z-0.264}{z-0.896}
\end{equation}


\subsubsection{Настройка нейросетевого регулятора}
\subbbsection{Предварительное обучение нейросети}
\subbbsection{Обучение нейросети в контуре управления}

\subsubsection{Влияние качества НС--О на скорость обучения НС--Р в контуре}
